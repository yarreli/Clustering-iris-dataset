


---
por [Yareli](https://github.com/yarreli)



# Cluster jer√°rquico flores de iris

_La idea general del cl√∫ster jer√°rquico se basa en la distancia entre los puntos del conjunto de datos. 
Hay dos formas de hacer clustering jer√°rquico: Agglomerative y Divisive. Para inferir el n√∫mero de subgrupos 
en el conjunto se usa el dendograma.<br>
El conjunto de datos flor de iris contiene 50 observaciones de cada una de las 3 variedades de iris: setosa, 
virginica y versicolor. Estamos interesados en aplicar 3 algoritmos de agrupamiento y comparar su 
desempe√±o, adem√°s de medir la "cophenetic correlation" entre cada resultado del agrupamiento, la cual
es una medida de cu√°n fielmente un dendrograma preserva las distancias por pares entre el conjunto de datos._

### Explorando los datos
Previo a la clusterizaci√≥n, se hace un an√°lisis para familiarizarse con los datos por medio de estad√≠sticos
descriptivos y t√©cnicas de visualizaci√≥n. Adem√°s de  analizar la presencia de outliers.

## Cl√∫ster jer√°rquico üñáÔ∏è
El grupamiento empleado fue *Agglomerative*, en donde se forman grupos de abajo hacia arriba, y
hay 4 formas de linking (vincular) los datos: Ward, Complete, Single y Average. <br>
Primero se gener√≥ el dendograma en donde la distancia entre las barras representa la distancia al 
siguiente centro del grupo, esta t√©cnica de visualizaci√≥n ayuda a determinar el n√∫mero de subgrupos
a formar. <br>
A pesar de lo que sugieran los dendogramas, se eligieron 3 cl√∫ster en cada uno de los tres m√©todos, ya
que sabemos que hay tres variedades de flor, sin embargo, esto no siempre ocurre en la realidad.

## Pasos en el m√©todo de clustering jer√°rquico
1. Dibujar el dendograma
2. Generar el cl√∫ster: utilizando la funci√≥n de python:
```
AgglomerativeClustering(n_clusters=k, affinity="euclidean", linkage="ward")
```
### Determinar los m√©todos a comparar
La funci√≥n anterior se aplic√≥ a todas las posibles combinaciones de los m√©todos, en donde se pueden combinar affinity y linkage para conseguir m√©todos con precisiones diferentes y se 
elegir√≠a el de la mejor precisi√≥n. A continuaci√≥n se muestran algunos ejemplos de las posibles combinaciones.
```
#for affin in ['euclidean','manhattan', 'cosine']:     
for affin in ['euclidean','manhattan']:
    for link in ['complete', 'single', 'average']:
        Hclus = AgglomerativeClustering(n_clusters=3, affinity=affin, linkage=link)
        Hclus.fit(xx)
        print ("Accuracy for a model with affinity=", affin, "and linkage=", link, ":")      
        print ("%.2f" % sm.accuracy_score(vari.variety_num, Hclus.labels_) )  # accuracy
        
Accuracy for a model with affinity=euclidean and linkage= ward: 0.89
Accuracy for a model with affinity= euclidean and linkage= complete: 0.49
Accuracy for a model with affinity= euclidean and linkage= single: 0.68
Accuracy for a model with affinity= euclidean and linkage= average: 0.91
Accuracy for a model with affinity= manhattan and linkage= complete: 0.33
Accuracy for a model with affinity= manhattan and linkage= single: 0.67
Accuracy for a model with affinity= manhattan and linkage= average: 0.90
Accuracy for a model with affinity= cosine and linkage= average: 0.66
```
Despu√©s de lo anterior se eligieron tres m√©todos para analizarlos a m√°s profundidad, se eligieron de tal manera que
tuvieran precisiones alejadas entre s√≠.

## Resultados
### M√©todo 1: Euclidean y Ward 
En este m√©todo, la distancia para el √∫ltimo subgrupo es de 21 unidades aproximadamente (12 y 33), para el pen√∫ltimo fue
de 6 unidades, el anterior de 1, y las siguientes mucho menores. Por lo que en este caso tendr√≠a 
sentido elegir 2 o 3 cl√∫ster.

### Metodo 2: Euclidean y Complete
Aqu√≠, los grupos se encontraron separados en el rango de 4 a 7 unidades en distancia, es decir aproximadamente 3 unidades
los separan antes del √∫ltimo grupo formado. Un grupo previo se separa entre los puntos 2.5 a 3, menos definidos que en el caso anterior. Aqu√≠ es menos claro que se forman los 3 grupos que existen en realidad.

### M√©todo 3: Euclidean y Single
En este caso es claro que un grupo s√≠ est√° bien definido, pero los otros dos no claramente. Dado que no hay distancias tan distintas entre los grupos que se forman, se puede interpretar que aqu√≠ puede haber mayor error de agrupamiento ya que los grupos son m√°s similares entre s√≠.

### Desempe√±o de los m√©todos
Al evaluar el desempe√±o de los tres modelos se hace con la tasa de √©xitos, que fue calculada como la proporci√≥n de los elementos bien claificados respecto al total. El que trabaja mejor es el que usa linkage=ward.<br>
El Cophenetic Correlation Coefficient(CPCC) es una medida de la bondad de ajuste del cl√∫ster, para obtenerlo se necesita calcular la correlaci√≥n entre la matriz de distancias y la "Cophenetic matrix", esta √∫ltima es la distancia de los datos originales en el dendograma. Con esta medida se concluye lo mismo que con la anterior; los m√©todos *ward*, *complete* y *single* se desempe√±an de mejor a menor en ese orden.
```
Success ratio and cophenetic correlation:
                   accuracy     cophe
Ward Linkage      89.333333  0.872828
Complete Linkage  84.000000  0.726986
Single Linkage    68.000000  0.863879
```
En los tres agrupamientos el CCPP se comport√≥ de manera similar que la tasa de √©xitos. Por lo tanto, es obvio que elegir√© el Ward Linkage como modelo de agrupaci√≥n jer√°rquica para el conjunto de datos de Iris.
