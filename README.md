
(affinity="euclidean", linkage='ward')   # accuracy = 0.8933

(affinity="euclidean", linkage='single')  # 0.68
(affinity="euclidean", linkage='average') # 0.9066
(affinity="euclidean", linkage='complete') #0.4933

(affinity="manhattan", linkage='single')  # 0.6733
(affinity="manhattan", linkage='average') # 0.9
(affinity="cosine", linkage='average')    # 0.66
 
### M√©todo 1: Ward
La distancia para el √∫ltimo subgrupo es de 21 unidades aproximadamente (12 y 33), para el pen√∫ltimo fue
de 6 unidades, el anterior de 1, y las siguientes mucho menores. Por lo que en este caso tendr√≠a 
sentido elegir 2 o 3 cl√∫ster.

### Metodo 2: Complete
de 4 a 7 unidades en distancia antes del √∫ltimo grupo formado, 2.5 a 3, menos definidos que en el caso anterior.

### M√©todo 3: Single
0.8 a 1.6  0.78 a 0.8
Es claro que un grupo s√≠ est√° bien definido, pero los otros dos no claramente.

menos definidoooooo

Obtengo el puntaje de precisi√≥n m√°s alto de 0.68 cuando se usa Euclidean y el pAverage como par√°metros de enlace. 
Por lo tanto, es obvio que elegir√© el tercero como modelo de agrupaci√≥n jer√°rquica para el conjunto de datos de Iris.




---
por [yareli](https://github.com/yarreli)



# Cluster jer√°rquico flores de iris

_La idea general del cl√∫ster jer√°rquico se basa en la distancia entre los puntos del conjunto de datos. 
Hay dos formas de hacer clustering jer√°rquico: Agglomerative y Divisive. Para inferir el n√∫mero de subgrupos 
en el conjunto se usa el dendograma.<br>
El conjunto de datos contiene 50 observaciones de cada una de las 3 variedades de iris: setosa, 
virginica y versicolor. Estamos interesados en aplicar 3 algoritmos de agrupamiento y comparar su 
desempe√±o, adem√°s de medir la "cophenetic correlation" entre cada resultado del agrupamiento, la cual
es una medida de cu√°n fielmente un dendrograma preserva las distancias por pares entre el conjunto de datos._

### Explorando los datos
Previo a la clusterizaci√≥n, se hace un an√°lisis para familiarizarse con los datos por medio de estad√≠sticos
descriptivos y t√©cnicas de visualizaci√≥n. Adem√°s de  an√°lizar la presencia de outliers.

## Cl√∫ster jer√°rquico üñáÔ∏è
La forma de agrupamiento fue con *Agglomerative*, en donde se forman grupos de abajo hacia arriba, y
hay 4 formas de linking (vincular) los datos: Ward, Complete, Single y Average. <br>
Primero se gener√≥ el dendograma en donde la distancia entre las barras representa la distancia al 
siguiente centro del grupo, esta t√©cnica de visualizaci√≥n ayuda a determinar el n√∫mero de subgrupos
a formar. <br>
A pesar de lo que sugieran los dendogramas, se eligieron 3 cl√∫ster en cada uno de los tres m√©todos, ya
que sabemos que hay tres variedades de flor, sin embargo, esto no siempre ocurre en la realidad.

## Pasos en el m√©todo de clustering jer√°rquico
1. Dibujar el dendograma
2. Generar el cl√∫ster: utilizando la funci√≥n de python:
```
AgglomerativeClustering(n_clusters=k, affinity="euclidean", linkage="ward")
```
donde se pueden combinar affinity y linkage para conseguir m√©todos con precisiones diferentes y se 
elegir√≠a el de la mejor precisi√≥n. A continuaci√≥n se muestran 3 ejemplos de las posibles combinaciones.

```
Hclus = AgglomerativeClustering(n_clusters=3, affinity="manhattan", linkage='average')
Hclus.fit(flors)
sm.accuracy_score(vari.variety_num, Hclus.labels_)    # accuracy
# 0.9

Hclus = AgglomerativeClustering(n_clusters=3, affinity="euclidean", linkage='ward')
Hclus.fit(flors)
sm.accuracy_score(vari.variety_num, Hclus.labels_)      # accuracy
# 0.8933

Hclus = AgglomerativeClustering(n_clusters=3, affinity="euclidean", linkage='single')
Hclus.fit(flors)
sm.accuracy_score(vari.variety_num, Hclus.labels_)      # accuracy
# 0.68
```

