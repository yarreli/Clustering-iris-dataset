


---
por [yareli](https://github.com/yarreli)



# Cluster jer√°rquico flores de iris

_La idea general del cl√∫ster jer√°rquico se basa en la distancia entre los puntos del conjunto de datos. 
Hay dos formas de hacer clustering jer√°rquico: Agglomerative y Divisive. Para inferir el n√∫mero de subgrupos 
en el conjunto se usa el dendograma.<br>
El conjunto de datos flor de iris contiene 50 observaciones de cada una de las 3 variedades de iris: setosa, 
virginica y versicolor. Estamos interesados en aplicar 3 algoritmos de agrupamiento y comparar su 
desempe√±o, adem√°s de medir la "cophenetic correlation" entre cada resultado del agrupamiento, la cual
es una medida de cu√°n fielmente un dendrograma preserva las distancias por pares entre el conjunto de datos._

### Explorando los datos
Previo a la clusterizaci√≥n, se hace un an√°lisis para familiarizarse con los datos por medio de estad√≠sticos
descriptivos y t√©cnicas de visualizaci√≥n. Adem√°s de  analizar la presencia de outliers.

## Cl√∫ster jer√°rquico üñáÔ∏è
El grupamiento empleado fue *Agglomerative*, en donde se forman grupos de abajo hacia arriba, y
hay 4 formas de linking (vincular) los datos: Ward, Complete, Single y Average. <br>
Primero se gener√≥ el dendograma en donde la distancia entre las barras representa la distancia al 
siguiente centro del grupo, esta t√©cnica de visualizaci√≥n ayuda a determinar el n√∫mero de subgrupos
a formar. <br>
A pesar de lo que sugieran los dendogramas, se eligieron 3 cl√∫ster en cada uno de los tres m√©todos, ya
que sabemos que hay tres variedades de flor, sin embargo, esto no siempre ocurre en la realidad.

## Pasos en el m√©todo de clustering jer√°rquico
1. Dibujar el dendograma
2. Generar el cl√∫ster: utilizando la funci√≥n de python:
```
AgglomerativeClustering(n_clusters=k, affinity="euclidean", linkage="ward")
```
### Determinar los m√©todos a comparar
La funci√≥n anterior se aplic√≥ a todas las posibles combinaciones de los m√©todos, en donde se pueden combinar affinity y linkage para conseguir m√©todos con precisiones diferentes y se 
elegir√≠a el de la mejor precisi√≥n. A continuaci√≥n se muestran algunos ejemplos de las posibles combinaciones.
```
#for affin in ['euclidean','manhattan', 'cosine']:     
for affin in ['euclidean','manhattan']:
    for link in ['complete', 'single', 'average']:
        Hclus = AgglomerativeClustering(n_clusters=3, affinity=affin, linkage=link)
        Hclus.fit(xx)
        print ("Accuracy for a model with affinity=", affin, "and linkage=", link, ":")      
        print ("%.2f" % sm.accuracy_score(vari.variety_num, Hclus.labels_) )  # accuracy
        
Accuracy for a model with affinity=euclidean and linkage= ward: 0.89
Accuracy for a model with affinity= euclidean and linkage= complete: 0.49
Accuracy for a model with affinity= euclidean and linkage= single: 0.68
Accuracy for a model with affinity= euclidean and linkage= average: 0.91
Accuracy for a model with affinity= manhattan and linkage= complete: 0.33
Accuracy for a model with affinity= manhattan and linkage= single: 0.67
Accuracy for a model with affinity= manhattan and linkage= average: 0.90
Accuracy for a model with affinity= cosine and linkage= average: 0.66
```



 
### M√©todo 1: Ward
La distancia para el √∫ltimo subgrupo es de 21 unidades aproximadamente (12 y 33), para el pen√∫ltimo fue
de 6 unidades, el anterior de 1, y las siguientes mucho menores. Por lo que en este caso tendr√≠a 
sentido elegir 2 o 3 cl√∫ster.

### Metodo 2: Complete
de 4 a 7 unidades en distancia antes del √∫ltimo grupo formado, 2.5 a 3, menos definidos que en el caso anterior.

### M√©todo 3: Single
0.8 a 1.6  0.78 a 0.8
Es claro que un grupo s√≠ est√° bien definido, pero los otros dos no claramente.

menos definidoooooo

Obtengo el puntaje de precisi√≥n m√°s alto de 0.68 cuando se usa Euclidean y el pAverage como par√°metros de enlace. 
Por lo tanto, es obvio que elegir√© el tercero como modelo de agrupaci√≥n jer√°rquica para el conjunto de datos de Iris.


